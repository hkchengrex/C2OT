defaults:
  - base_config
  - override hydra/job_logging: custom
  - _self_

hydra:
  run:
    dir: ./output/imagenet32/${exp_id}
  output_subdir: train-${now:%Y-%m-%d_%H-%M-%S}-hydra


dataset: imagenet32
num_classes: null
size: 32
data_path: ../data/imagenet/train_blurred_32/memmap

num_gen: 49997

clip_path: ../data/imagenet/train_clip_captions.pth
val_clip_path: ../data/imagenet/val_clip_captions.pth
conditional: True
conditioning_code: continuous
code_dim: 1024

# fm / ot / ot-cls
fm_type: fm
cls_weight: 100.0
target_r: 0.2

model: unet-imagenet

# ema configuration
ema_decay: 0.9999

num_iterations: 300_000
learning_rate: 1.0e-4
linear_warmup_steps: 20_000

log_text_interval: 200
log_extra_interval: 50_000
eval_interval: 300_000
save_checkpoint_interval: 30_000
save_copy_iterations: []
save_copy_always: True

eval_batch_size: 512 # per-GPU
batch_size: 512 # global, aka effective; default: 4 GPUs
# How large should the OT batch be compared to the network batch?
# With 4 GPUs, 512/4*50=6400 given in the paper
oversample_ratio: 50

lr_schedule: linear

clip_grad_norm: 1.0
weight_decay: 0.0
dropout: 0.0